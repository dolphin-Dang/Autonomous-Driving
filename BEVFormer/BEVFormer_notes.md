[一文读懂BEVFormer -- 知乎](https://zhuanlan.zhihu.com/p/538490215)

# 概述
BEVFormer主要解决一个特征级融合问题，从纯camera数据得到BEV视角。得到整体的特征空间之后可以完成各项下游任务。

在3D空间之外，考虑时间维度，形成四维时空。时空维度可以辅助预测物体速度、轨迹，也可以辅助猜想。

特征融合过程中可能遇到的问题：
+ 自身运动补偿：车本身在运动，不同时刻的特征需要对齐
+ 时间差异：不同传感器的时间差
+ 空间差异：最后映射到同一个坐标系，空间位置要对齐
+ 以上的对齐并不是人工完成的，而是交给模型自己完成。

# BEVFormer
### 输入格式
张量 (batch_size, queue, camera, C, H, W)
+ 批次大小
+ queue：连续帧的个数，主要解决遮挡问题
+ camera：每一帧包含的图像数量（相机个数），nuScenes数据集中为6
  + 前后左右
  + 左前，右前

### Backbone
从六个视角分别抽特征。尽量选取速度快的骨干网络即可。后续待使用。

### 时间注意力模块
自注意力机制：自己和自己其他时刻的信息做注意力。

实际上是一个DeformableAttention，可变形attention。
+ 通过FC层找到几个偏移量和对应的权重，仅和这几个偏移量的位置计算注意力并求加权和。
+ 多头中，每个head都有自己的偏移量和权重
+ 具体的偏移个数并不是越多越好

### 空间注意力机制
cross-attention：融合多个视角的特征。

3D空间的点要投影到2D的BEV空间，对应点只是一个参考，仍需要使用DeformableAttention在对应点的附近进行采样。

如果每一个query都和六个视角计算attention就太慢了，通过映射找到对应关系，减少query计算attention的数量。

### 后续发展
+ 多尺度特征
+ 多个检测器集成

### 实现细节
训练阶段：从过去两秒的连续序列中随机取出3个，称为t-3、t-2、t-1，和现在的t。其中t-3的时间自注意力机制退化为自注意力机制。计算完之后是一个feature map，输入到检测和分割头部，计算相应的损失。


# nuScenes数据库介绍
### Basics
[nuScenes的官方网站](https://www.nuscenes.org/)。

[nuScenes论文](https://arxiv.org/pdf/1903.11027.pdf)。


### New Bing
> promt：介绍nuScenes数据库
> 
> New Bing：
> nuScenes是一个公开的大规模自动驾驶数据集。它的数据采集主要在波士顿和新加坡进行，用于采集的车辆装备了1个旋转雷达（spinning LIDAR），5个远程雷达传感器（long range RADAR sensor）和6个相机（camera）。
> nuScenes数据库的主要特点包括：
> - **传感器的配备**：一共配备了6个相机、一个LiDAR、5个RADAR。6个相机的分辨率都是1600x900，除了背后的相机FOV为110度，其他的5个相机的FOV为70度，前置相机和侧面的相机的视野中线角度为55度。相机的采集速率是12Hz。用了一个32线的LiDAR，采集速率是20Hz。
> - **数据标注**：对于每一个场景，选择其中的关键帧做标注，标注速率在2Hz，总共标注了23类物体的3D bbox（ x, y, z, width, length, height and yaw angle）、类别信息、以及其他的一些属性 (可见度、状态)。
> - **评价指标**：对于物体检测问题，对于t时刻的场景，我们可以使用 [t-0.5,t]内的传感器数据去检测场景内的物体。并且我们只是选择了23种物体中的10种作为检测对象。在评测时依旧使用检测领域的AP，不过AP的阈值匹配不使用IoU来计算，而使用在地平面上的2D中心距离d来计算。
> 
> 这个数据库可以让研究者研究具有挑战性的城市驾驶情况，使用真实自动驾驶汽车的全套传感器。更多详细信息，你可以访问[nuScenes的官方网站](https://www.nuscenes.org/)。
